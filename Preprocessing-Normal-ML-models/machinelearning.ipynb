{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('embedding_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>-0.012397</td>\n",
       "      <td>-0.023947</td>\n",
       "      <td>-0.020647</td>\n",
       "      <td>0.057316</td>\n",
       "      <td>-0.017636</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>-0.039695</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.007169</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>-0.108725</td>\n",
       "      <td>-0.083004</td>\n",
       "      <td>-0.088404</td>\n",
       "      <td>0.048190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.063068</td>\n",
       "      <td>0.038138</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.057476</td>\n",
       "      <td>-0.058234</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>-0.041539</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>-0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052351</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>-0.050997</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>0.099606</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.039040</td>\n",
       "      <td>0.020087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065330</td>\n",
       "      <td>0.110592</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>-0.031124</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.043514</td>\n",
       "      <td>-0.013266</td>\n",
       "      <td>0.035069</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.085088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057267</td>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-0.054332</td>\n",
       "      <td>0.019138</td>\n",
       "      <td>-0.022098</td>\n",
       "      <td>-0.063703</td>\n",
       "      <td>-0.041810</td>\n",
       "      <td>-0.064544</td>\n",
       "      <td>0.026752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004673</td>\n",
       "      <td>-0.061435</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.018435</td>\n",
       "      <td>0.087143</td>\n",
       "      <td>-0.039585</td>\n",
       "      <td>0.168794</td>\n",
       "      <td>0.048899</td>\n",
       "      <td>-0.027572</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>-0.054455</td>\n",
       "      <td>0.030207</td>\n",
       "      <td>0.049191</td>\n",
       "      <td>-0.008918</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>-0.082972</td>\n",
       "      <td>-0.009210</td>\n",
       "      <td>0.049869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069091</td>\n",
       "      <td>-0.067866</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>-0.022631</td>\n",
       "      <td>0.075540</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>-0.069418</td>\n",
       "      <td>-0.109697</td>\n",
       "      <td>0.056782</td>\n",
       "      <td>-0.018839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023595</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.082110</td>\n",
       "      <td>0.066656</td>\n",
       "      <td>-0.117361</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590731</th>\n",
       "      <td>-0.045651</td>\n",
       "      <td>0.046932</td>\n",
       "      <td>0.071737</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>0.122472</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.072465</td>\n",
       "      <td>0.019906</td>\n",
       "      <td>0.049194</td>\n",
       "      <td>-0.005351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055703</td>\n",
       "      <td>-0.084585</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>-0.054505</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.095907</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.107358</td>\n",
       "      <td>0.086660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590732</th>\n",
       "      <td>-0.108527</td>\n",
       "      <td>-0.056124</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>0.049788</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>-0.007260</td>\n",
       "      <td>-0.053514</td>\n",
       "      <td>-0.079950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.018965</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>-0.048818</td>\n",
       "      <td>-0.095453</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>-0.096453</td>\n",
       "      <td>0.025704</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590733</th>\n",
       "      <td>-0.058433</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>-0.019677</td>\n",
       "      <td>0.082006</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>-0.007329</td>\n",
       "      <td>0.029408</td>\n",
       "      <td>-0.018967</td>\n",
       "      <td>-0.093649</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002869</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.135897</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>-0.013728</td>\n",
       "      <td>-0.070462</td>\n",
       "      <td>-0.064188</td>\n",
       "      <td>0.031530</td>\n",
       "      <td>-0.040046</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590734</th>\n",
       "      <td>0.036119</td>\n",
       "      <td>0.088556</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.064540</td>\n",
       "      <td>0.063516</td>\n",
       "      <td>-0.059856</td>\n",
       "      <td>-0.035248</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069314</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>0.123259</td>\n",
       "      <td>0.073292</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.079983</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590735</th>\n",
       "      <td>-0.083478</td>\n",
       "      <td>0.053237</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>-0.085669</td>\n",
       "      <td>0.049535</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.015994</td>\n",
       "      <td>0.019224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025295</td>\n",
       "      <td>0.037148</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.048850</td>\n",
       "      <td>0.163131</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>0.097843</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1590736 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.007757  0.065417  0.025096 -0.012397 -0.023947 -0.020647  0.057316   \n",
       "1       -0.004413  0.063068  0.038138  0.023968  0.057476 -0.058234 -0.034942   \n",
       "2        0.065330  0.110592 -0.019685 -0.031124  0.005653  0.043514 -0.013266   \n",
       "3       -0.004673 -0.061435  0.039444  0.018435  0.087143 -0.039585  0.168794   \n",
       "4        0.069091 -0.067866  0.006624 -0.022631  0.075540  0.006292 -0.069418   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1590731 -0.045651  0.046932  0.071737  0.036617  0.122472  0.005156  0.072465   \n",
       "1590732 -0.108527 -0.056124 -0.009704  0.015547  0.040445  0.049788  0.007453   \n",
       "1590733 -0.058433  0.051736 -0.019677  0.082006 -0.010967 -0.007329  0.029408   \n",
       "1590734  0.036119  0.088556  0.042343 -0.003442  0.007299  0.064540  0.063516   \n",
       "1590735 -0.083478  0.053237 -0.005245 -0.017661  0.003863 -0.085669  0.049535   \n",
       "\n",
       "                7         8         9  ...       375       376       377  \\\n",
       "0       -0.017636  0.062745  0.016968  ...  0.002670 -0.039695 -0.008833   \n",
       "1       -0.041539  0.006695 -0.025274  ... -0.052351  0.015281  0.026596   \n",
       "2        0.035069  0.051432  0.085088  ...  0.057267 -0.032742 -0.054332   \n",
       "3        0.048899 -0.027572 -0.004275  ... -0.035368 -0.054455  0.030207   \n",
       "4       -0.109697  0.056782 -0.018839  ... -0.023595  0.003362  0.011569   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1590731  0.019906  0.049194 -0.005351  ... -0.055703 -0.084585  0.022677   \n",
       "1590732 -0.007260 -0.053514 -0.079950  ...  0.005902  0.018965 -0.007838   \n",
       "1590733 -0.018967 -0.093649 -0.011248  ... -0.002869  0.015314  0.135897   \n",
       "1590734 -0.059856 -0.035248  0.043376  ... -0.069314 -0.008713  0.012621   \n",
       "1590735 -0.001711 -0.015994  0.019224  ... -0.025295  0.037148  0.049449   \n",
       "\n",
       "              378       379       380       381       382       383  target  \n",
       "0       -0.007169  0.009722 -0.108725 -0.083004 -0.088404  0.048190       0  \n",
       "1       -0.050997  0.037713  0.099606 -0.009366 -0.039040  0.020087       0  \n",
       "2        0.019138 -0.022098 -0.063703 -0.041810 -0.064544  0.026752       0  \n",
       "3        0.049191 -0.008918  0.068192 -0.082972 -0.009210  0.049869       0  \n",
       "4       -0.010151  0.082110  0.066656 -0.117361 -0.001433  0.038626       0  \n",
       "...           ...       ...       ...       ...       ...       ...     ...  \n",
       "1590731 -0.054505  0.041781  0.095907  0.003328 -0.107358  0.086660       4  \n",
       "1590732 -0.048818 -0.095453  0.057588  0.023268 -0.096453  0.025704       4  \n",
       "1590733  0.041178 -0.013728 -0.070462 -0.064188  0.031530 -0.040046       4  \n",
       "1590734  0.035547 -0.002681  0.123259  0.073292 -0.003762 -0.079983       4  \n",
       "1590735  0.008464  0.048850  0.163131  0.072825  0.007459  0.097843       4  \n",
       "\n",
       "[1590736 rows x 385 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1590731    4\n",
       "1590732    4\n",
       "1590733    4\n",
       "1590734    4\n",
       "1590735    4\n",
       "Name: target, Length: 1590736, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>-0.012397</td>\n",
       "      <td>-0.023947</td>\n",
       "      <td>-0.020647</td>\n",
       "      <td>0.057316</td>\n",
       "      <td>-0.017636</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032273</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>-0.039695</td>\n",
       "      <td>-0.008833</td>\n",
       "      <td>-0.007169</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>-0.108725</td>\n",
       "      <td>-0.083004</td>\n",
       "      <td>-0.088404</td>\n",
       "      <td>0.048190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.063068</td>\n",
       "      <td>0.038138</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.057476</td>\n",
       "      <td>-0.058234</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>-0.041539</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>-0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>-0.052351</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>-0.050997</td>\n",
       "      <td>0.037713</td>\n",
       "      <td>0.099606</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.039040</td>\n",
       "      <td>0.020087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065330</td>\n",
       "      <td>0.110592</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>-0.031124</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.043514</td>\n",
       "      <td>-0.013266</td>\n",
       "      <td>0.035069</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.085088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098777</td>\n",
       "      <td>0.057267</td>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-0.054332</td>\n",
       "      <td>0.019138</td>\n",
       "      <td>-0.022098</td>\n",
       "      <td>-0.063703</td>\n",
       "      <td>-0.041810</td>\n",
       "      <td>-0.064544</td>\n",
       "      <td>0.026752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004673</td>\n",
       "      <td>-0.061435</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.018435</td>\n",
       "      <td>0.087143</td>\n",
       "      <td>-0.039585</td>\n",
       "      <td>0.168794</td>\n",
       "      <td>0.048899</td>\n",
       "      <td>-0.027572</td>\n",
       "      <td>-0.004275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>-0.054455</td>\n",
       "      <td>0.030207</td>\n",
       "      <td>0.049191</td>\n",
       "      <td>-0.008918</td>\n",
       "      <td>0.068192</td>\n",
       "      <td>-0.082972</td>\n",
       "      <td>-0.009210</td>\n",
       "      <td>0.049869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069091</td>\n",
       "      <td>-0.067866</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>-0.022631</td>\n",
       "      <td>0.075540</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>-0.069418</td>\n",
       "      <td>-0.109697</td>\n",
       "      <td>0.056782</td>\n",
       "      <td>-0.018839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.023595</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.082110</td>\n",
       "      <td>0.066656</td>\n",
       "      <td>-0.117361</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>0.038626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590731</th>\n",
       "      <td>-0.045651</td>\n",
       "      <td>0.046932</td>\n",
       "      <td>0.071737</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>0.122472</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.072465</td>\n",
       "      <td>0.019906</td>\n",
       "      <td>0.049194</td>\n",
       "      <td>-0.005351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064682</td>\n",
       "      <td>-0.055703</td>\n",
       "      <td>-0.084585</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>-0.054505</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.095907</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.107358</td>\n",
       "      <td>0.086660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590732</th>\n",
       "      <td>-0.108527</td>\n",
       "      <td>-0.056124</td>\n",
       "      <td>-0.009704</td>\n",
       "      <td>0.015547</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>0.049788</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>-0.007260</td>\n",
       "      <td>-0.053514</td>\n",
       "      <td>-0.079950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.018965</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>-0.048818</td>\n",
       "      <td>-0.095453</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>-0.096453</td>\n",
       "      <td>0.025704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590733</th>\n",
       "      <td>-0.058433</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>-0.019677</td>\n",
       "      <td>0.082006</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>-0.007329</td>\n",
       "      <td>0.029408</td>\n",
       "      <td>-0.018967</td>\n",
       "      <td>-0.093649</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026139</td>\n",
       "      <td>-0.002869</td>\n",
       "      <td>0.015314</td>\n",
       "      <td>0.135897</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>-0.013728</td>\n",
       "      <td>-0.070462</td>\n",
       "      <td>-0.064188</td>\n",
       "      <td>0.031530</td>\n",
       "      <td>-0.040046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590734</th>\n",
       "      <td>0.036119</td>\n",
       "      <td>0.088556</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.064540</td>\n",
       "      <td>0.063516</td>\n",
       "      <td>-0.059856</td>\n",
       "      <td>-0.035248</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>-0.069314</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>0.123259</td>\n",
       "      <td>0.073292</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.079983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590735</th>\n",
       "      <td>-0.083478</td>\n",
       "      <td>0.053237</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>-0.085669</td>\n",
       "      <td>0.049535</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.015994</td>\n",
       "      <td>0.019224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>-0.025295</td>\n",
       "      <td>0.037148</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.048850</td>\n",
       "      <td>0.163131</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>0.097843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1590736 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "0        0.007757  0.065417  0.025096 -0.012397 -0.023947 -0.020647  0.057316   \n",
       "1       -0.004413  0.063068  0.038138  0.023968  0.057476 -0.058234 -0.034942   \n",
       "2        0.065330  0.110592 -0.019685 -0.031124  0.005653  0.043514 -0.013266   \n",
       "3       -0.004673 -0.061435  0.039444  0.018435  0.087143 -0.039585  0.168794   \n",
       "4        0.069091 -0.067866  0.006624 -0.022631  0.075540  0.006292 -0.069418   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1590731 -0.045651  0.046932  0.071737  0.036617  0.122472  0.005156  0.072465   \n",
       "1590732 -0.108527 -0.056124 -0.009704  0.015547  0.040445  0.049788  0.007453   \n",
       "1590733 -0.058433  0.051736 -0.019677  0.082006 -0.010967 -0.007329  0.029408   \n",
       "1590734  0.036119  0.088556  0.042343 -0.003442  0.007299  0.064540  0.063516   \n",
       "1590735 -0.083478  0.053237 -0.005245 -0.017661  0.003863 -0.085669  0.049535   \n",
       "\n",
       "                7         8         9  ...       374       375       376  \\\n",
       "0       -0.017636  0.062745  0.016968  ...  0.032273  0.002670 -0.039695   \n",
       "1       -0.041539  0.006695 -0.025274  ...  0.018440 -0.052351  0.015281   \n",
       "2        0.035069  0.051432  0.085088  ...  0.098777  0.057267 -0.032742   \n",
       "3        0.048899 -0.027572 -0.004275  ...  0.012330 -0.035368 -0.054455   \n",
       "4       -0.109697  0.056782 -0.018839  ... -0.000521 -0.023595  0.003362   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1590731  0.019906  0.049194 -0.005351  ...  0.064682 -0.055703 -0.084585   \n",
       "1590732 -0.007260 -0.053514 -0.079950  ...  0.011543  0.005902  0.018965   \n",
       "1590733 -0.018967 -0.093649 -0.011248  ... -0.026139 -0.002869  0.015314   \n",
       "1590734 -0.059856 -0.035248  0.043376  ...  0.034950 -0.069314 -0.008713   \n",
       "1590735 -0.001711 -0.015994  0.019224  ...  0.002585 -0.025295  0.037148   \n",
       "\n",
       "              377       378       379       380       381       382       383  \n",
       "0       -0.008833 -0.007169  0.009722 -0.108725 -0.083004 -0.088404  0.048190  \n",
       "1        0.026596 -0.050997  0.037713  0.099606 -0.009366 -0.039040  0.020087  \n",
       "2       -0.054332  0.019138 -0.022098 -0.063703 -0.041810 -0.064544  0.026752  \n",
       "3        0.030207  0.049191 -0.008918  0.068192 -0.082972 -0.009210  0.049869  \n",
       "4        0.011569 -0.010151  0.082110  0.066656 -0.117361 -0.001433  0.038626  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "1590731  0.022677 -0.054505  0.041781  0.095907  0.003328 -0.107358  0.086660  \n",
       "1590732 -0.007838 -0.048818 -0.095453  0.057588  0.023268 -0.096453  0.025704  \n",
       "1590733  0.135897  0.041178 -0.013728 -0.070462 -0.064188  0.031530 -0.040046  \n",
       "1590734  0.012621  0.035547 -0.002681  0.123259  0.073292 -0.003762 -0.079983  \n",
       "1590735  0.049449  0.008464  0.048850  0.163131  0.072825  0.007459  0.097843  \n",
       "\n",
       "[1590736 rows x 384 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1590736,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1272588 samples\n",
      "Test set size: 318148 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'embeds' is your embeddings DataFrame (1590736 rows × 384 columns)\n",
    "# and 'target' is the target column (1590736 rows)\n",
    "\n",
    "# Combine embeddings and target into a single DataFrame for easy splitting\n",
    "\n",
    "# Define the features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one (features)\n",
    "y = data.iloc[:, -1]   # The last column (target)\n",
    "\n",
    "# Perform train-test split with stratification to maintain label balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the sizes of the splits to verify\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74    159113\n",
      "           4       0.74      0.76      0.75    159035\n",
      "\n",
      "    accuracy                           0.75    318148\n",
      "   macro avg       0.75      0.75      0.75    318148\n",
      "weighted avg       0.75      0.75      0.75    318148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7236\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72    159113\n",
      "           4       0.72      0.73      0.73    159035\n",
      "\n",
      "    accuracy                           0.72    318148\n",
      "   macro avg       0.72      0.72      0.72    318148\n",
      "weighted avg       0.72      0.72      0.72    318148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you already have X_train, X_test, y_train, y_test from your previous split\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Initialize the SVM model with a linear kernel\n",
    "# svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# svm.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_pred = svm.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(svm, 'svm_model.joblib')\n",
    "# print(\"Model saved as 'svm_model.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Adjust 'categorical_crossentropy' for multiclass\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_pred_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Adjust input_shape as per features in X_train\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Use 'softmax' and adjust output layer if you have more than two classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Adjust 'categorical_crossentropy' for multiclass\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Threshold for binary classification\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
