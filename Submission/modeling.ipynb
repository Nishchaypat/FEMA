{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary libraries\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array, asarray, zeros\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Activation, Dropout, Dense, Flatten, GlobalMaxPooling1D, Embedding, \n",
    "    Conv1D, LSTM, SimpleRNN, Bidirectional, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# TensorFlow Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "# TensorFlow Configuration\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('embedding_target.csv')\n",
    "embedding = data.drop(columns=['target'])\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'embeds' is your embeddings DataFrame (1590736 rows Ã— 384 columns)\n",
    "# and 'target' is the target column (1590736 rows)\n",
    "\n",
    "# Combine embeddings and target into a single DataFrame for easy splitting\n",
    "\n",
    "# Define the features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one (features)\n",
    "y = data.iloc[:, -1]   # The last column (target)\n",
    "\n",
    "# Perform train-test split with stratification to maintain label balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the sizes of the splits to verify\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'penalty' : ['none', 'l2']\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(lr, lr_param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000, **grid_search.best_params_)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Re-evaluate\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you already have X_train, X_test, y_train, y_test from your previous split\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_param_grid = {\n",
    "    'var_smoothing' : np.logspace(0,-9, num=20),\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "grid_search = GridSearchCV(gnb, gnb_param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(**grid_search.best_params_)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Re-evaluate\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_grid = {\n",
    "    'C': [0.01, 1, 10, 20],\n",
    "    \"gamma\": [0.01, 0.1]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "svc_model = SVC()\n",
    "grid_search = GridSearchCV(svc_model, svc_param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "# Best hyperparameters: {'C': 20, 'gamma': 0.01}\n",
    "# Best cross-validation score: 0.7310000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(**grid_search.best_params_)\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Re-evaluate\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, knn_param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Re-evaluate\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Modeling and Hypertuning\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data.csv', encoding='utf-8', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "data.drop(columns=['ids', 'date', 'flag', 'user'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = shuffle(data, random_state=42).reset_index(drop=True)\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'target' to bnary sentiment labels (0 or 1)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df['text'].values, test_df['target'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the pooled output from the BERT encoder\n",
    "    net = outputs['pooled_output']\n",
    "    print(net)\n",
    "    # Add dropout for regularization\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    \n",
    "    # Add additional dense layers with ReLU activation\n",
    "    net = tf.keras.layers.Dense(128, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)  # Add dropout to the new dense layer\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(16, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    \n",
    "    # Final output layer with sigmoid activation for binary classification\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[text_input], outputs=[net])\n",
    "\n",
    "# Initialize and build the model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original shape\n",
    "original_shape = df.shape\n",
    "print(\"Original DataFrame shape:\", original_shape)\n",
    "\n",
    "# Calculate the new size (1/100 of the original)\n",
    "new_size = original_shape[0] // 100  # integer division to get the whole number\n",
    "print(\"New size for training data:\", new_size)\n",
    "\n",
    "# Randomly sample the training data\n",
    "smaller_train_df = train_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_train_df.shape)\n",
    "\n",
    "smaller_train_data = tf.data.Dataset.from_tensor_slices((smaller_train_df['text'].values, smaller_train_df['target'].values))\n",
    "\n",
    "smaller_test_df = test_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_test_df.shape)\n",
    "\n",
    "smaller_test_data = tf.data.Dataset.from_tensor_slices((smaller_test_df['text'].values, smaller_test_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_data =smaller_train_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = smaller_test_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(train_data, epochs=5)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "model.save('Test_79', include_optimizer=False)\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('Test_79', custom_objects={'KerasLayer': hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Hypermodel definition\n",
    "def build_hypermodel(hp):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "    \n",
    "    # BERT pooled output\n",
    "    net = outputs['pooled_output']\n",
    "    print(net)\n",
    "    # Add dense layers with hyperparameter tuning\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):  # Tune 1 to 3 additional layers\n",
    "        net = tf.keras.layers.Dense(\n",
    "            units=hp.Choice(f\"units_{i}\", [32, 64, 128]),  # Tune size per layer\n",
    "            activation='relu'\n",
    "        )(net)\n",
    "        net = tf.keras.layers.Dropout(hp.Float(f\"dropout_{i}\", 0.1, 0.5, step=0.1))(net)\n",
    "    \n",
    "    # Output layer\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs=[text_input], outputs=[net])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-5, 3e-5, 1e-4])\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "tuner = kt.Hyperband(\n",
    "    build_hypermodel,\n",
    "    objective='accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='hyperband_dir',\n",
    "    project_name='text_sentiment_analysis'\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_data, epochs=5)\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Summary of the best model\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Information\n",
    "\n",
    "## Trial ID\n",
    "- **ID:** 0020\n",
    "\n",
    "## Hyperparameters\n",
    "- **Hyperparameter Space:**\n",
    "  - `num_layers`: \n",
    "    - Type: Integer\n",
    "    - Range: [1, 3]\n",
    "  - `units_0`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_0`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "  - `learning_rate`: \n",
    "    - Type: Choice\n",
    "    - Options: [1e-05, 3e-05, 0.0001]\n",
    "    - Default: 1e-05\n",
    "  - `units_1`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_1`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "  - `units_2`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_2`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "\n",
    "- **Selected Values:**\n",
    "  - `num_layers`: 1\n",
    "  - `units_0`: 128\n",
    "  - `dropout_0`: 0.2\n",
    "  - `learning_rate`: 0.0001\n",
    "  - `units_1`: 128\n",
    "  - `dropout_1`: 0.2\n",
    "  - `units_2`: 64\n",
    "  - `dropout_2`: 0.4\n",
    "\n",
    "## Tuner Information\n",
    "- **Epochs:** 10\n",
    "- **Initial Epoch:** 4\n",
    "- **Bracket:** 1\n",
    "- **Round:** 1\n",
    "- **Trial ID:** 0017\n",
    "\n",
    "## Metrics\n",
    "- **Loss:**\n",
    "  - Direction: Minimize\n",
    "  - Observations: \n",
    "    - Value: 0.3865 at Step 5\n",
    "- **Accuracy:**\n",
    "  - Direction: Maximize\n",
    "  - Observations: \n",
    "    - Value: 0.8277 at Step 5\n",
    "\n",
    "## Score\n",
    "- **Best Score:** 0.8277\n",
    "- **Best Step:** 5\n",
    "- **Status:** Completed\n",
    "- **Message:** None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Modeling and Hypertuning\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embedding_matrix = np.loadtxt('embedding_matrix_lstm.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "\n",
    "# Embedding layer (use pre-trained embeddings like GloVe or FastText, fine-tune during training)\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "\n",
    "# Bidirectional LSTM Layer (captures context from both ends of the sentence)\n",
    "lstm_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "\n",
    "# Attention Layer (self-attention)\n",
    "# Pass the same tensor as both query and value\n",
    "attention_output = Attention()([lstm_model.output, lstm_model.output])\n",
    "\n",
    "# Global Average Pooling to reduce the output dimensionality\n",
    "lstm_model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Dropout Layer (regularization to prevent overfitting)\n",
    "lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Dense Layer (final classification layer)\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_model_history.history['accuracy'])\n",
    "plt.plot(lstm_model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lstm_model_history.history['loss'])\n",
    "plt.plot(lstm_model_history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    # Define the model architecture\n",
    "    lstm_model = Sequential()\n",
    "\n",
    "    # Embedding layer (use pre-trained embeddings like GloVe or FastText, fine-tune during training)\n",
    "    embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    lstm_model.add(embedding_layer)\n",
    "\n",
    "    # Bidirectional LSTM Layer (captures context from both ends of the sentence)\n",
    "    lstm_model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=128, max_value=240, step=32), \n",
    "        return_sequences=True)))\n",
    "\n",
    "    # Attention Layer (self-attention)\n",
    "    attention_output = Attention()([lstm_model.output, lstm_model.output])\n",
    "\n",
    "    # Global Average Pooling to reduce the output dimensionality\n",
    "    lstm_model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # Dropout Layer (regularization to prevent overfitting)\n",
    "    lstm_model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)))\n",
    "\n",
    "    # Dense Layer (final classification layer)\n",
    "    lstm_model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=180, step=32),\n",
    "        activation='relu'))\n",
    "    lstm_model.add(Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1)))\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with an optimizer\n",
    "    lstm_model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',  # Use binary crossentropy for binary classification tasks\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return lstm_model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,  # Number of trials to run\n",
    "    executions_per_trial=1,  # Run each trial once\n",
    "    directory='my_dir_2',  # Save results here\n",
    "    project_name='lstm_bayesian_tuning_2'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=2)\n",
    "\n",
    "# Get the best hyperparameterss\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hp.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('best_lstm_model_new.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters: {'lstm_units': 192, 'dropout_rate': 0.5, 'dense_units': 128, 'dropout_rate_2': 0.30000000000000004, 'learning_rate': 0.0010872353209015178}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
