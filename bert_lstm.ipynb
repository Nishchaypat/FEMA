{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "data.drop(columns=['ids', 'date', 'flag', 'user'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0</td>\n",
       "      <td>this song's middle change just doesn't want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>@officialnjonas Good luck with that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0</td>\n",
       "      <td>@ProudGamerTweet I rather average 32370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0</td>\n",
       "      <td>Pickin up @misstinayao waitin on @sadittysash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>0</td>\n",
       "      <td>@ home studying for maths wooot ! im so going ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0             0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
       "1             0  @misstoriblack cool , i have no tweet apps  fo...\n",
       "2             0  @TiannaChaos i know  just family drama. its la...\n",
       "3             0  School email won't open  and I have geography ...\n",
       "4             0                             upper airways problem \n",
       "...         ...                                                ...\n",
       "1599995       0  this song's middle change just doesn't want to...\n",
       "1599996       4               @officialnjonas Good luck with that \n",
       "1599997       0           @ProudGamerTweet I rather average 32370 \n",
       "1599998       0  Pickin up @misstinayao waitin on @sadittysash ...\n",
       "1599999       0  @ home studying for maths wooot ! im so going ...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data, random_state=42).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'target' to bnary sentiment labels (0 or 1)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df['text'].values, test_df['target'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (1600000, 2)\n",
      "New size for training data: 16000\n",
      "Smaller Training DataFrame shape: (16000, 2)\n",
      "Smaller Training DataFrame shape: (16000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Original shape\n",
    "original_shape = df.shape\n",
    "print(\"Original DataFrame shape:\", original_shape)\n",
    "\n",
    "# Calculate the new size (1/100 of the original)\n",
    "new_size = original_shape[0] // 100 # integer division to get the whole number\n",
    "print(\"New size for training data:\", new_size)\n",
    "\n",
    "# Randomly sample the training data\n",
    "smaller_train_df = train_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_train_df.shape)\n",
    "\n",
    "smaller_train_data = tf.data.Dataset.from_tensor_slices((smaller_train_df['text'].values, smaller_train_df['target'].values))\n",
    "\n",
    "smaller_test_df = test_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_test_df.shape)\n",
    "\n",
    "smaller_test_data = tf.data.Dataset.from_tensor_slices((smaller_test_df['text'].values, smaller_test_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "bert_model_url = \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\"  # SST-2 model trained for sentiment analysis\n",
    "preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "bert_encoder = hub.KerasLayer(bert_model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer[1][0]',            \n",
      "                                None, 768),                       'keras_layer[1][1]',            \n",
      "                                 'sequence_output':               'keras_layer[1][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128, 256)    918528      ['keras_layer_1[1][14]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 128, 256)     0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 256)         0           ['attention_1[0][0]']            \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 16)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            17          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,438,338\n",
      "Trainable params: 956,097\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model():\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(units=128, return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=0.3)(pooled_output)  # Set dropout rate as 0.3\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(128, activation='relu')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(64, activation='tanh')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(32, activation='relu')(net)  # Second dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    net = Dense(16, activation='relu')(net)  # Third dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    # Final output layer with sigmoid activation for binary classification\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),  # Set learning rate to 1e-4\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "500/500 [==============================] - 1020s 2s/step - loss: 0.5821 - accuracy: 0.6855\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 1066s 2s/step - loss: 0.5181 - accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289efd8a080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# Train the model\n",
    "model.fit(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [01h 34m 32s]\n",
      "accuracy: 0.6755937337875366\n",
      "\n",
      "Best accuracy So Far: 0.7838749885559082\n",
      "Total elapsed time: 15h 44m 29s\n",
      "Best Hyperparameters: {'lstm_units': 40, 'dropout_rate': 0.5, 'dense_units': 52, 'dropout_rate_2': 0.30000000000000004, 'learning_rate': 0.00027081735801911806}\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=20, max_value=50, step=5),\n",
    "        return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1))(pooled_output)\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=36, max_value=120, step=16),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=12, max_value=30, step=4),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)  # Final output layer for binary classification\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,  # Number of trials to run\n",
    "    executions_per_trial=2,  # Run each trial once\n",
    "    directory='BERT_LSTM_new',  # Save results here\n",
    "    project_name='bert_lstm_attention_tuning'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hp.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 1228s 5s/step - loss: 0.5249 - accuracy: 0.7478\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 1271s 5s/step - loss: 0.4744 - accuracy: 0.7791\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 1277s 5s/step - loss: 0.4624 - accuracy: 0.7907\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 1247s 5s/step - loss: 0.4489 - accuracy: 0.7928\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 1249s 5s/step - loss: 0.4389 - accuracy: 0.8020\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 1253s 5s/step - loss: 0.4309 - accuracy: 0.8059\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 1248s 5s/step - loss: 0.4217 - accuracy: 0.8104\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 1250s 5s/step - loss: 0.4179 - accuracy: 0.8122\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 1243s 5s/step - loss: 0.4110 - accuracy: 0.8173\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 1247s 5s/step - loss: 0.4004 - accuracy: 0.8215\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 1256s 5s/step - loss: 0.3942 - accuracy: 0.8255\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 1253s 5s/step - loss: 0.3834 - accuracy: 0.8319\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 1267s 5s/step - loss: 0.3779 - accuracy: 0.8332\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 1250s 5s/step - loss: 0.3631 - accuracy: 0.8379\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 1259s 5s/step - loss: 0.3532 - accuracy: 0.8444\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 1313s 5s/step - loss: 0.3464 - accuracy: 0.8486\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 1272s 5s/step - loss: 0.3351 - accuracy: 0.8533\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 1265s 5s/step - loss: 0.3231 - accuracy: 0.8581\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 1270s 5s/step - loss: 0.3082 - accuracy: 0.8717\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 1272s 5s/step - loss: 0.2933 - accuracy: 0.8741\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 1277s 5s/step - loss: 0.2852 - accuracy: 0.8774\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 1274s 5s/step - loss: 0.2851 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 1276s 5s/step - loss: 0.2921 - accuracy: 0.8770\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 1256s 5s/step - loss: 0.2902 - accuracy: 0.8761\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 1260s 5s/step - loss: 0.2800 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 1284s 5s/step - loss: 0.2691 - accuracy: 0.8908\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 1305s 5s/step - loss: 0.2520 - accuracy: 0.8939\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 1315s 5s/step - loss: 0.2371 - accuracy: 0.9018\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 1277s 5s/step - loss: 0.2608 - accuracy: 0.8932\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 1276s 5s/step - loss: 0.2610 - accuracy: 0.8914\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 1343s 5s/step - loss: 0.2431 - accuracy: 0.8990\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 1668s 7s/step - loss: 0.2388 - accuracy: 0.9051\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 1475s 6s/step - loss: 0.2166 - accuracy: 0.9091\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 1290s 5s/step - loss: 0.1969 - accuracy: 0.9225\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 1256s 5s/step - loss: 0.1741 - accuracy: 0.9307\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 1253s 5s/step - loss: 0.1659 - accuracy: 0.9340\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 1263s 5s/step - loss: 0.1659 - accuracy: 0.9361\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 1316s 5s/step - loss: 0.1738 - accuracy: 0.9308\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 1305s 5s/step - loss: 0.1557 - accuracy: 0.9382\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 1271s 5s/step - loss: 0.1903 - accuracy: 0.9261\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 1311s 5s/step - loss: 0.1557 - accuracy: 0.9381\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 1304s 5s/step - loss: 0.1380 - accuracy: 0.9473\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 1310s 5s/step - loss: 0.1270 - accuracy: 0.9525\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 1314s 5s/step - loss: 0.1555 - accuracy: 0.9399\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 1280s 5s/step - loss: 0.1457 - accuracy: 0.9443\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 1275s 5s/step - loss: 0.1356 - accuracy: 0.9474\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 1278s 5s/step - loss: 0.1298 - accuracy: 0.9504\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 1279s 5s/step - loss: 0.1601 - accuracy: 0.9370\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 1273s 5s/step - loss: 0.1190 - accuracy: 0.9553\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 1275s 5s/step - loss: 0.1054 - accuracy: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2165fb46830>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(best_hp)\n",
    "model.fit(smaller_train_df['text'], smaller_train_df['target'], epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14/10000 [..............................] - ETA: 8:28:41"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(test_df['text'])\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "y_true = test_df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7937\n",
      "Precision: 0.7955\n",
      "Recall: 0.7916\n",
      "F1 Score: 0.7936\n",
      "Confusion Matrix:\n",
      "[[6354 1631]\n",
      " [1670 6345]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 938ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999977]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"I am happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "model.save('LSTM_BERT/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
