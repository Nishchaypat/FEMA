{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "data.drop(columns=['ids', 'date', 'flag', 'user'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0</td>\n",
       "      <td>this song's middle change just doesn't want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>@officialnjonas Good luck with that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0</td>\n",
       "      <td>@ProudGamerTweet I rather average 32370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0</td>\n",
       "      <td>Pickin up @misstinayao waitin on @sadittysash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>0</td>\n",
       "      <td>@ home studying for maths wooot ! im so going ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0             0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
       "1             0  @misstoriblack cool , i have no tweet apps  fo...\n",
       "2             0  @TiannaChaos i know  just family drama. its la...\n",
       "3             0  School email won't open  and I have geography ...\n",
       "4             0                             upper airways problem \n",
       "...         ...                                                ...\n",
       "1599995       0  this song's middle change just doesn't want to...\n",
       "1599996       4               @officialnjonas Good luck with that \n",
       "1599997       0           @ProudGamerTweet I rather average 32370 \n",
       "1599998       0  Pickin up @misstinayao waitin on @sadittysash ...\n",
       "1599999       0  @ home studying for maths wooot ! im so going ...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data, random_state=42).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'target' to bnary sentiment labels (0 or 1)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df['text'].values, test_df['target'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (1600000, 2)\n",
      "New size for training data: 16000\n",
      "Smaller Training DataFrame shape: (16000, 2)\n",
      "Smaller Training DataFrame shape: (16000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Original shape\n",
    "original_shape = df.shape\n",
    "print(\"Original DataFrame shape:\", original_shape)\n",
    "\n",
    "# Calculate the new size (1/100 of the original)\n",
    "new_size = original_shape[0] // 100 # integer division to get the whole number\n",
    "print(\"New size for training data:\", new_size)\n",
    "\n",
    "# Randomly sample the training data\n",
    "smaller_train_df = train_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_train_df.shape)\n",
    "\n",
    "smaller_train_data = tf.data.Dataset.from_tensor_slices((smaller_train_df['text'].values, smaller_train_df['target'].values))\n",
    "\n",
    "smaller_test_df = test_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_test_df.shape)\n",
    "\n",
    "smaller_test_data = tf.data.Dataset.from_tensor_slices((smaller_test_df['text'].values, smaller_test_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "bert_model_url = \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\"  # SST-2 model trained for sentiment analysis\n",
    "preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "bert_encoder = hub.KerasLayer(bert_model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer[1][0]',            \n",
      "                                None, 768),                       'keras_layer[1][1]',            \n",
      "                                 'sequence_output':               'keras_layer[1][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128, 256)    918528      ['keras_layer_1[1][14]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 128, 256)     0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 256)         0           ['attention_1[0][0]']            \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 16)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            17          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,438,338\n",
      "Trainable params: 956,097\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model():\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(units=128, return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=0.3)(pooled_output)  # Set dropout rate as 0.3\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(128, activation='relu')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(64, activation='tanh')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(32, activation='relu')(net)  # Second dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    net = Dense(16, activation='relu')(net)  # Third dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    # Final output layer with sigmoid activation for binary classification\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),  # Set learning rate to 1e-4\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "500/500 [==============================] - 1020s 2s/step - loss: 0.5821 - accuracy: 0.6855\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 1066s 2s/step - loss: 0.5181 - accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289efd8a080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# Train the model\n",
    "model.fit(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "30                |30                |lstm_units\n",
      "0.6               |0.6               |dropout_rate\n",
      "84                |84                |dense_units\n",
      "0.5               |0.5               |dropout_rate_2\n",
      "1.3317e-05        |1.3317e-05        |learning_rate\n",
      "\n",
      "Epoch 1/2\n",
      "  7/500 [..............................] - ETA: 23:23 - loss: 0.7681 - accuracy: 0.4688"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=20, max_value=50, step=5),\n",
    "        return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1))(pooled_output)\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=36, max_value=120, step=16),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=12, max_value=30, step=4),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)  # Final output layer for binary classification\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,  # Number of trials to run\n",
    "    executions_per_trial=2,  # Run each trial once\n",
    "    directory='BERT_LSTM_new',  # Save results here\n",
    "    project_name='bert_lstm_attention_tuning'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hp.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 128s 5s/step - loss: 0.5674 - accuracy: 0.7081\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.5048 - accuracy: 0.7613\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.4840 - accuracy: 0.7819\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 126s 5s/step - loss: 0.4562 - accuracy: 0.8037\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.4311 - accuracy: 0.7950\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.4116 - accuracy: 0.8194\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.4105 - accuracy: 0.8125\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.3787 - accuracy: 0.8369\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.3637 - accuracy: 0.8381\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.3595 - accuracy: 0.8469\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.3458 - accuracy: 0.8531\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 125s 5s/step - loss: 0.3284 - accuracy: 0.8600\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.2951 - accuracy: 0.8669\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.3062 - accuracy: 0.8625\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.2564 - accuracy: 0.8919\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.2858 - accuracy: 0.8881\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.2381 - accuracy: 0.9031\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.2304 - accuracy: 0.9044\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 125s 5s/step - loss: 0.1992 - accuracy: 0.9187\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.1780 - accuracy: 0.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2160afb9d80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(best_hp)\n",
    "model.fit(smaller_train_df['text'], smaller_train_df['target'], epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 284s 3s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(smaller_test_df['text'])\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "y_true = smaller_test_df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7450\n",
      "Precision: 0.7805\n",
      "Recall: 0.6645\n",
      "F1 Score: 0.7178\n",
      "Confusion Matrix:\n",
      "[[1346  292]\n",
      " [ 524 1038]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 365ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9989673]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"I am happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
