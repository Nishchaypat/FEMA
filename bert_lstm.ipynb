{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "data.drop(columns=['ids', 'date', 'flag', 'user'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>upper airways problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0</td>\n",
       "      <td>this song's middle change just doesn't want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>@officialnjonas Good luck with that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0</td>\n",
       "      <td>@ProudGamerTweet I rather average 32370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0</td>\n",
       "      <td>Pickin up @misstinayao waitin on @sadittysash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>0</td>\n",
       "      <td>@ home studying for maths wooot ! im so going ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "0             0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
       "1             0  @misstoriblack cool , i have no tweet apps  fo...\n",
       "2             0  @TiannaChaos i know  just family drama. its la...\n",
       "3             0  School email won't open  and I have geography ...\n",
       "4             0                             upper airways problem \n",
       "...         ...                                                ...\n",
       "1599995       0  this song's middle change just doesn't want to...\n",
       "1599996       4               @officialnjonas Good luck with that \n",
       "1599997       0           @ProudGamerTweet I rather average 32370 \n",
       "1599998       0  Pickin up @misstinayao waitin on @sadittysash ...\n",
       "1599999       0  @ home studying for maths wooot ! im so going ...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data, random_state=42).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'target' to bnary sentiment labels (0 or 1)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df['text'].values, test_df['target'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (1600000, 2)\n",
      "New size for training data: 1600\n",
      "Smaller Training DataFrame shape: (1600, 2)\n",
      "Smaller Training DataFrame shape: (1600, 2)\n"
     ]
    }
   ],
   "source": [
    "# Original shape\n",
    "original_shape = df.shape\n",
    "print(\"Original DataFrame shape:\", original_shape)\n",
    "\n",
    "# Calculate the new size (1/100 of the original)\n",
    "new_size = original_shape[0] // 1000  # integer division to get the whole number\n",
    "print(\"New size for training data:\", new_size)\n",
    "\n",
    "# Randomly sample the training data\n",
    "smaller_train_df = train_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_train_df.shape)\n",
    "\n",
    "smaller_train_data = tf.data.Dataset.from_tensor_slices((smaller_train_df['text'].values, smaller_train_df['target'].values))\n",
    "\n",
    "smaller_test_df = test_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_test_df.shape)\n",
    "\n",
    "smaller_test_data = tf.data.Dataset.from_tensor_slices((smaller_test_df['text'].values, smaller_test_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "bert_model_url = \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\"  # SST-2 model trained for sentiment analysis\n",
    "preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "bert_encoder = hub.KerasLayer(bert_model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_type_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer[1][0]',            \n",
      "                                None, 768),                       'keras_layer[1][1]',            \n",
      "                                 'sequence_output':               'keras_layer[1][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128, 256)    918528      ['keras_layer_1[1][14]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 128, 256)     0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 256)         0           ['attention_1[0][0]']            \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 16)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            17          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,438,338\n",
      "Trainable params: 956,097\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Attention, GlobalAveragePooling1D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model():\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(units=128, return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=0.3)(pooled_output)  # Set dropout rate as 0.3\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(128, activation='relu')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(64, activation='tanh')(net)  # First dense layer\n",
    "    net = Dropout(rate=0.4)(net)  # Dropout for regularization\n",
    "    net = Dense(32, activation='relu')(net)  # Second dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    net = Dense(16, activation='relu')(net)  # Third dense layer\n",
    "    net = Dropout(rate=0.4)(net)\n",
    "    \n",
    "    # Final output layer with sigmoid activation for binary classification\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),  # Set learning rate to 1e-4\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "500/500 [==============================] - 1020s 2s/step - loss: 0.5821 - accuracy: 0.6855\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 1066s 2s/step - loss: 0.5181 - accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289efd8a080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# Train the model\n",
    "model.fit(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from BERT_LSTM\\bert_lstm_attention_tuning\\tuner0.json\n",
      "Best Hyperparameters: {'lstm_units': 176, 'dropout_rate': 0.30000000000000004, 'dense_units': 112, 'dropout_rate_2': 0.4, 'learning_rate': 0.00014525853640695105}\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    # Input layer for text\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    # BERT preprocessor and encoder (assuming you have a pre-trained BERT model)\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the sequence output from the BERT encoder (not just pooled output)\n",
    "    bert_sequence_output = outputs['sequence_output']\n",
    "    \n",
    "    # Add a Bidirectional LSTM Layer\n",
    "    lstm_output = Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=20, max_value=50, step=5),\n",
    "        return_sequences=True))(bert_sequence_output)\n",
    "    \n",
    "    # Attention Layer for sequence processing\n",
    "    attention_output = Attention()([lstm_output, lstm_output])\n",
    "    \n",
    "    # Apply Global Average Pooling to the attention output\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    net = Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1))(pooled_output)\n",
    "\n",
    "    # Add Dense Layers\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=36, max_value=120, step=16),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(\n",
    "        units=hp.Int('dense_units', min_value=12, max_value=30, step=4),\n",
    "        activation='relu')(net)\n",
    "    net = Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.7, step=0.1))(net)\n",
    "    net = Dense(1, activation='sigmoid', name='classifier')(net)  # Final output layer for binary classification\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=net)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',  # Binary classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=10,  # Number of trials to run\n",
    "    executions_per_trial=2,  # Run each trial once\n",
    "    directory='BERT_LSTM_new',  # Save results here\n",
    "    project_name='bert_lstm_attention_tuning'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(smaller_train_df['text'], smaller_train_df['target'], epochs=2, batch_size=32)  # Use a reasonable batch size\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hp.values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
