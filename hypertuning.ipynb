{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "data.drop(columns=['ids', 'date', 'flag', 'user'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data, random_state=42).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert data to TensorFlow datasets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues))\n",
      "File \u001b[0;32m~/School/CSC4790-data-science/project/FEMA/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/School/CSC4790-data-science/project/FEMA/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/School/CSC4790-data-science/project/FEMA/.venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2419\u001b[0m     )\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Convert 'target' to bnary sentiment labels (0 or 1)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_df['text'].values, test_df['target'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_outliersremoved.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_url = \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\"  # SST-2 model trained for sentiment analysis\n",
    "preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "bert_encoder = hub.KerasLayer(bert_model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "    # Extract the pooled output from the BERT encoder\n",
    "    net = outputs['pooled_output']\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    \n",
    "    # Add additional dense layers with ReLU activation\n",
    "    net = tf.keras.layers.Dense(128, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)  # Add dropout to the new dense layer\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(16, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    \n",
    "    # Final output layer with sigmoid activation for binary classification\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[text_input], outputs=[net])\n",
    "\n",
    "# Initialize and build the model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (1600000, 2)\n",
      "New size for training data: 16000\n",
      "Smaller Training DataFrame shape: (16000, 2)\n",
      "Smaller Training DataFrame shape: (16000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Original shape\n",
    "original_shape = df.shape\n",
    "print(\"Original DataFrame shape:\", original_shape)\n",
    "\n",
    "# Calculate the new size (1/100 of the original)\n",
    "new_size = original_shape[0] // 100  # integer division to get the whole number\n",
    "print(\"New size for training data:\", new_size)\n",
    "\n",
    "# Randomly sample the training data\n",
    "smaller_train_df = train_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_train_df.shape)\n",
    "\n",
    "smaller_train_data = tf.data.Dataset.from_tensor_slices((smaller_train_df['text'].values, smaller_train_df['target'].values))\n",
    "\n",
    "smaller_test_df = test_df.sample(n=new_size, random_state=42)\n",
    "\n",
    "# Check the shape of the new training dataset\n",
    "print(\"Smaller Training DataFrame shape:\", smaller_test_df.shape)\n",
    "\n",
    "smaller_test_data = tf.data.Dataset.from_tensor_slices((smaller_test_df['text'].values, smaller_test_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 65s 129ms/step - loss: 0.4492 - accuracy: 0.7991\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 65s 129ms/step - loss: 0.4492 - accuracy: 0.7986\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 65s 130ms/step - loss: 0.4447 - accuracy: 0.8021\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 65s 130ms/step - loss: 0.4520 - accuracy: 0.7949\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.4430 - accuracy: 0.8023\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_data =smaller_train_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = smaller_test_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(train_data, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 67s 132ms/step - loss: 0.4646 - accuracy: 0.7909\n",
      "Test Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 360). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "model.save('Test_79', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('Test_79', custom_objects={'KerasLayer': hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "HYPERTUNNING\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 Complete [00h 11m 01s]\n",
      "accuracy: 0.7975624799728394\n",
      "\n",
      "Best accuracy So Far: 0.8276875019073486\n",
      "Total elapsed time: 04h 50m 05s\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_8 (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_9 (KerasLayer)     {'default': (None,   109482241   ['keras_layer_8[32][0]',         \n",
      "                                768),                             'keras_layer_8[32][1]',         \n",
      "                                 'pooled_output': (               'keras_layer_8[32][2]']         \n",
      "                                None, 768),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768)}                                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['keras_layer_9[32][13]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 1)            129         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,580,802\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Hypermodel definition\n",
    "def build_hypermodel(hp):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessed_text = bert_preprocessor(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "    \n",
    "    # BERT pooled output\n",
    "    net = outputs['pooled_output']\n",
    "    \n",
    "    # Add dense layers with hyperparameter tuning\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):  # Tune 1 to 3 additional layers\n",
    "        net = tf.keras.layers.Dense(\n",
    "            units=hp.Choice(f\"units_{i}\", [32, 64, 128]),  # Tune size per layer\n",
    "            activation='relu'\n",
    "        )(net)\n",
    "        net = tf.keras.layers.Dropout(hp.Float(f\"dropout_{i}\", 0.1, 0.5, step=0.1))(net)\n",
    "    \n",
    "    # Output layer\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs=[text_input], outputs=[net])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-5, 3e-5, 1e-4])\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning setup\n",
    "tuner = kt.Hyperband(\n",
    "    build_hypermodel,\n",
    "    objective='accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory='hyperband_dir',\n",
    "    project_name='text_sentiment_analysis'\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(train_data, epochs=5)\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Summary of the best model\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "BEST PARAMETERS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Information\n",
    "\n",
    "## Trial ID\n",
    "- **ID:** 0020\n",
    "\n",
    "## Hyperparameters\n",
    "- **Hyperparameter Space:**\n",
    "  - `num_layers`: \n",
    "    - Type: Integer\n",
    "    - Range: [1, 3]\n",
    "  - `units_0`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_0`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "  - `learning_rate`: \n",
    "    - Type: Choice\n",
    "    - Options: [1e-05, 3e-05, 0.0001]\n",
    "    - Default: 1e-05\n",
    "  - `units_1`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_1`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "  - `units_2`: \n",
    "    - Type: Choice\n",
    "    - Options: [32, 64, 128]\n",
    "    - Default: 32\n",
    "  - `dropout_2`: \n",
    "    - Type: Float\n",
    "    - Range: [0.1, 0.5]\n",
    "    - Default: 0.1\n",
    "\n",
    "- **Selected Values:**\n",
    "  - `num_layers`: 1\n",
    "  - `units_0`: 128\n",
    "  - `dropout_0`: 0.2\n",
    "  - `learning_rate`: 0.0001\n",
    "  - `units_1`: 128\n",
    "  - `dropout_1`: 0.2\n",
    "  - `units_2`: 64\n",
    "  - `dropout_2`: 0.4\n",
    "\n",
    "## Tuner Information\n",
    "- **Epochs:** 10\n",
    "- **Initial Epoch:** 4\n",
    "- **Bracket:** 1\n",
    "- **Round:** 1\n",
    "- **Trial ID:** 0017\n",
    "\n",
    "## Metrics\n",
    "- **Loss:**\n",
    "  - Direction: Minimize\n",
    "  - Observations: \n",
    "    - Value: 0.3865 at Step 5\n",
    "- **Accuracy:**\n",
    "  - Direction: Maximize\n",
    "  - Observations: \n",
    "    - Value: 0.8277 at Step 5\n",
    "\n",
    "## Score\n",
    "- **Best Score:** 0.8277\n",
    "- **Best Step:** 5\n",
    "- **Status:** Completed\n",
    "- **Message:** None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
